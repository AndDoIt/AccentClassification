{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8ohiXWadI_S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "\n",
    "n_epochs_Lenet = 10\n",
    "n_epochs_Alexnet = 50\n",
    "learning_rate = 0.03\n",
    "momentum = 0.8\n",
    "momentum_4 = 0.4\n",
    "train_size = 80\n",
    "test_size = 20\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vk7dG35pDEK"
   },
   "outputs": [],
   "source": [
    "## If using google colab, you'll need these two lines as well\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLBRAqhEpOdc"
   },
   "outputs": [],
   "source": [
    "## First, we load the CSV Files that contais the information about the samples registered for each native language\n",
    "english_data = pd.read_csv('img/english_subset.csv')\n",
    "mandarin_data = pd.read_csv('img/mandarin_subset.csv')\n",
    "korean_data = pd.read_csv('img/korean_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rE2K-LWxyYt"
   },
   "outputs": [],
   "source": [
    "## The objective of this class is to create items composed by a Tensor that represent and Image and a Tensor that represent a Label.\n",
    "## The inputs will be a csv File with the information needed for each sample and the directory of the images and the CSV.\n",
    "\n",
    "class LanClassDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.dataset = []\n",
    "        self.labels = csv_file  \n",
    "\n",
    "        for idx in range(len(self.labels)):\n",
    "          img_name = os.path.join(root_dir,\n",
    "                                self.labels['file_name'][idx])\n",
    "          image = io.imread(img_name)\n",
    "          transform = transforms.Compose([torchvision.transforms.ToPILImage(),\n",
    "                                          torchvision.transforms.Resize([32,64]),transforms.Grayscale(num_output_channels=1),transforms.ToTensor()])\n",
    "          image = transform(image)\n",
    "          self.dataset.append(image)\n",
    "\n",
    "        self.dataset = torch.stack(self.dataset)\n",
    "        self.labels,_ = pd.factorize(self.labels['native_language'])\n",
    "        self.labels = torch.from_numpy(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.size(self.label_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = self.dataset[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample\n",
    "\n",
    "    def __delitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        del self.dataset[idx]\n",
    "        del self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ue1wxj6P_OYH"
   },
   "outputs": [],
   "source": [
    "## The objective of this function is to randomly select for each native language the training and the test\n",
    "## samples. They will be added to the dataframes that will be used for the networks.\n",
    "\n",
    "def Random_selection_samples():\n",
    "\n",
    "  english_size = len(english_data)\n",
    "  korean_size = len(korean_data)\n",
    "  mandarin_size = len(mandarin_data)\n",
    "\n",
    "  from random import randint\n",
    "\n",
    "  english_vector = np.empty((0,1), int)\n",
    "  english_train_vector = np.empty((0,1), int)\n",
    "  english_test_vector = np.empty((0,1), int)\n",
    "  korean_vector = np.empty((0,1), int)\n",
    "  korean_train_vector = np.empty((0,1), int)\n",
    "  korean_test_vector = np.empty((0,1), int)\n",
    "  mandarin_vector = np.empty((0,1), int)\n",
    "  mandarin_train_vector = np.empty((0,1), int)\n",
    "  mandarin_test_vector = np.empty((0,1), int)\n",
    "\n",
    "  ## all the indexes of the objects available\n",
    "  for index in range (english_size):\n",
    "    english_vector = np.append(english_vector,index)\n",
    "  for index in range (korean_size):\n",
    "    korean_vector = np.append(korean_vector,index)\n",
    "  for index in range (mandarin_size):\n",
    "    mandarin_vector = np.append(mandarin_vector,index)\n",
    "\n",
    "  ## we randomly pick the train objects\n",
    "  for index in range (train_size):\n",
    "    value = randint(0, len(english_vector)-1)\n",
    "    english_train_vector = np.append(english_train_vector,english_vector[value])\n",
    "    english_vector = np.delete(english_vector,value)\n",
    "  for index in range (train_size):\n",
    "    value = randint(0, len(korean_vector)-1)\n",
    "    korean_train_vector = np.append(korean_train_vector,korean_vector[value])\n",
    "    korean_vector = np.delete(korean_vector,value)\n",
    "  for index in range (train_size):\n",
    "    value = randint(0, len(mandarin_vector)-1)\n",
    "    mandarin_train_vector = np.append(mandarin_train_vector,mandarin_vector[value])\n",
    "    mandarin_vector = np.delete(mandarin_vector,value)\n",
    "\n",
    "  ## we randomly pick the test objects\n",
    "  for index in range (test_size):\n",
    "    value = randint(0, len(english_vector)-1)\n",
    "    english_test_vector = np.append(english_test_vector,english_vector[value])\n",
    "    english_vector = np.delete(english_vector,value)\n",
    "  for index in range (test_size):\n",
    "    value = randint(0, len(korean_vector)-1)\n",
    "    korean_test_vector = np.append(korean_test_vector,korean_vector[value])\n",
    "    korean_vector = np.delete(korean_vector,value)\n",
    "  for index in range (test_size):\n",
    "    value = randint(0, len(mandarin_vector)-1)\n",
    "    mandarin_test_vector = np.append(mandarin_test_vector,mandarin_vector[value])\n",
    "    mandarin_vector = np.delete(mandarin_vector,value)\n",
    "\n",
    "  ## Now we add all the items to the dataframes\n",
    "  column_names = [\"file_name\", \"native_language\"]\n",
    "\n",
    "  df_train = pd.DataFrame(columns = column_names)\n",
    "  df_test = pd.DataFrame(columns = column_names)\n",
    "\n",
    "  ## First the test dataframe\n",
    "  for value in (english_test_vector):\n",
    "    df_test= df_test.append({'file_name' : english_data['file_name'][value] , 'native_language' : english_data['native_language'][value]} , ignore_index=True)\n",
    "  for value in (korean_test_vector):\n",
    "    df_test= df_test.append({'file_name' : korean_data['file_name'][value] , 'native_language' : korean_data['native_language'][value]} , ignore_index=True)\n",
    "  for value in (mandarin_test_vector):\n",
    "    df_test= df_test.append({'file_name' : mandarin_data['file_name'][value] , 'native_language' : mandarin_data['native_language'][value]} , ignore_index=True)\n",
    "\n",
    "  ## Then the train dataframe\n",
    "  for value in (english_train_vector):\n",
    "    df_train= df_train.append({'file_name' : english_data['file_name'][value] , 'native_language' : english_data['native_language'][value]} , ignore_index=True)\n",
    "  for value in (korean_train_vector):\n",
    "    df_train= df_train.append({'file_name' : korean_data['file_name'][value] , 'native_language' : korean_data['native_language'][value]} , ignore_index=True)\n",
    "  for value in (mandarin_train_vector):\n",
    "    df_train= df_train.append({'file_name' : mandarin_data['file_name'][value] , 'native_language' : mandarin_data['native_language'][value]} , ignore_index=True)\n",
    "\n",
    "  return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbYld61vudZ2"
   },
   "outputs": [],
   "source": [
    "## This function has the goal of creating the items of the train and test loaders\n",
    "def get_loaders():\n",
    "  train_loader = LanClassDataset(csv_file=df_train,\n",
    "                                      root_dir='img/spectograms/')\n",
    "  test_loader = LanClassDataset(csv_file=df_test,\n",
    "                                      root_dir='img/spectograms/')\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUSbNN7h6Fka"
   },
   "outputs": [],
   "source": [
    "## This network is the adaptation of the LeNet architecture.\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 15, 2)\n",
    "        self.fc1 = nn.Linear(15 * 7 * 15, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 15 * 7 * 15)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIR00pP4G-2c"
   },
   "outputs": [],
   "source": [
    "## This network is the adaptation of the AlexNet architecture.\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 28, kernel_size=2, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(28, 84, kernel_size=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(84, 168, kernel_size=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(168, 112, kernel_size=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(112, 112, kernel_size=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(112 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xj_v9cPX6HRY"
   },
   "outputs": [],
   "source": [
    "## We initialize the network, the optimizer and the criterion.\n",
    "network = AlexNet()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum = momentum)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVaeFgol6Xqp"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "  running_loss = 0.0\n",
    "  for i,_ in enumerate(train_loader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    images = train_loader.dataset\n",
    "    labels = train_loader.labels\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = network(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # we print the loos obtained at the end of each train epoch\n",
    "    running_loss += loss.item()\n",
    "    if i == train_size*3-1:\n",
    "      print('[%d, %5d] loss: %.3f' %\n",
    "        (epoch, i + 1, running_loss / 20))\n",
    "    running_loss = 0.0\n",
    "\n",
    "  ## the state of the network is saved at the end of the train epoch\n",
    "  torch.save(network.state_dict(), F\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeiq7-vt6fgm"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "  \n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for i,_ in enumerate(test_loader):\n",
    "        images = test_loader.dataset\n",
    "        labels = test_loader.labels\n",
    "        outputs = network(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test images: %.2f %%' % (\n",
    "      100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHNrYnsYbm-V"
   },
   "outputs": [],
   "source": [
    "def epoch(it_number):\n",
    "  print('Epoch number %d:' % (it_number))\n",
    "  train(it_number)\n",
    "  test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiMAsF7k6imP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation number 1:\n",
      "Random samples taken\n",
      "Data loaded\n",
      "Network = Lenet, Optimizer = Adam\n",
      "22:11:09.831213\n",
      "Epoch number 1:\n",
      "[1,   240] loss: 0.002\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:11:32.190844\n",
      "Epoch number 2:\n",
      "[2,   240] loss: 0.275\n",
      "Accuracy of the network on the test images: 33.33 %\n",
      "22:11:52.985443\n",
      "Epoch number 3:\n",
      "[3,   240] loss: 0.001\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:12:14.227103\n",
      "Epoch number 4:\n",
      "[4,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:12:35.465083\n",
      "Epoch number 5:\n",
      "[5,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:12:56.657192\n",
      "Epoch number 6:\n",
      "[6,   240] loss: 0.001\n",
      "Accuracy of the network on the test images: 81.67 %\n",
      "22:13:19.013320\n",
      "Epoch number 7:\n",
      "[7,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:13:43.392865\n",
      "Epoch number 8:\n",
      "[8,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 80.00 %\n",
      "22:14:08.100375\n",
      "Epoch number 9:\n",
      "[9,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 81.67 %\n",
      "22:14:30.648300\n",
      "Epoch number 10:\n",
      "[10,   240] loss: 0.000\n",
      "Accuracy of the network on the test images: 81.67 %\n",
      "22:14:53.400586\n",
      "Network = Alexnet, Optimizer = SGD Mom = 0.8\n",
      "22:14:53.718384\n",
      "Epoch number 1:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-615f1bcb7d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs_Alexnet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-71f01d270cb0>\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(it_number)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch number %d:'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-929df46cbc21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/dl/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/dl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 5 simulations will be run, to select different random samples. Each of the selections\n",
    "## will be tested for each of the Networks + Optimizers defined, to compare the \n",
    "## performance of each combination.\n",
    "\n",
    "for simulation in range(1,6):\n",
    "  print('Simulation number %d:' % (simulation))\n",
    "  df_train,df_test = Random_selection_samples()\n",
    "  print('Random samples taken')\n",
    "  train_loader, test_loader = get_loaders()\n",
    "  print('Data loaded')\n",
    "\n",
    "  print('Network = Alexnet, Optimizer = SGD Mom = 0.8')\n",
    "  network = AlexNet()\n",
    "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                        momentum = momentum)\n",
    "  criterion = nn.CrossEntropyLoss() \n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Alexnet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())\n",
    "\n",
    "  print('Network = Lenet, Optimizer = Adam')\n",
    "  network = LeNet()\n",
    "  optimizer = optim.Adam(network.parameters(), lr=0.001,\n",
    "                         betas=(0.9, 0.999),weight_decay = 0.002, \n",
    "                         amsgrad=False)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())\n",
    "\n",
    "  print('Network = Lenet, Optimizer = RMSProp')\n",
    "  network = LeNet()\n",
    "  optimizer = optim.RMSprop(network.parameters(), lr=0.001,\n",
    "                          alpha=0.99)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())\n",
    "\n",
    "  print('Network = Lenet, Optimizer = SGD Mom = 0.8')\n",
    "  network = LeNet()\n",
    "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                        momentum = momentum)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())\n",
    "\n",
    "  print('Network = Lenet, Optimizer = SGD Mom = 0.4')\n",
    "  network = LeNet()\n",
    "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                        momentum = momentum_4)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())\n",
    "\n",
    "  print('Network = Lenet, Optimizer = SGD without momentum')\n",
    "  network = LeNet()\n",
    "  optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(datetime.datetime.now().time())\n",
    "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
    "    epoch(epoch_number)\n",
    "    print(datetime.datetime.now().time())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LanClass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
