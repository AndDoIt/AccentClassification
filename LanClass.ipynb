{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanClass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ohiXWadI_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import os\n",
        "from skimage import io, transform\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "\n",
        "n_epochs_Lenet = 10\n",
        "n_epochs_Alexnet = 50\n",
        "learning_rate = 0.03\n",
        "momentum = 0.8\n",
        "momentum_4 = 0.4\n",
        "train_size = 80\n",
        "test_size = 20\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vk7dG35pDEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLBRAqhEpOdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## First, we load the CSV Files that contais the information about the samples registered for each native language\n",
        "english_data = pd.read_csv('/content/gdrive/My Drive/images/english_subset.csv')\n",
        "mandarin_data = pd.read_csv('/content/gdrive/My Drive/images/mandarin_subset.csv')\n",
        "korean_data = pd.read_csv('/content/gdrive/My Drive/images/korean_subset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rE2K-LWxyYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The objective of this class is to create items composed by a Tensor that represent and Image and a Tensor that represent a Label.\n",
        "## The inputs will be a csv File with the information needed for each sample and the directory of the images and the CSV.\n",
        "\n",
        "class LanClassDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \n",
        "        self.dataset = []\n",
        "        self.labels = csv_file  \n",
        "\n",
        "        for idx in range(len(self.labels)):\n",
        "          img_name = os.path.join(root_dir,\n",
        "                                self.labels['filename'][idx])\n",
        "          image = io.imread(img_name)\n",
        "          transform = transforms.Compose([torchvision.transforms.ToPILImage(),\n",
        "                                          torchvision.transforms.Resize([32,64]),transforms.Grayscale(num_output_channels=1),transforms.ToTensor()])\n",
        "          image = transform(image)\n",
        "          self.dataset.append(image)\n",
        "\n",
        "        self.dataset = torch.stack(self.dataset)\n",
        "        self.labels,_ = pd.factorize(self.labels['native_language'])\n",
        "        self.labels = torch.from_numpy(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.size(self.label_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image = self.dataset[idx]\n",
        "        label = self.labels[idx]\n",
        "        sample = {'image': image, 'label': label}\n",
        "        return sample\n",
        "\n",
        "    def __delitem__(self,idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        del self.dataset[idx]\n",
        "        del self.labels[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue1wxj6P_OYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The objective of this function is to randomly select for each native language the training and the test\n",
        "## samples. They will be added to the dataframes that will be used for the networks.\n",
        "\n",
        "def Random_selection_samples():\n",
        "\n",
        "  english_size = len(english_data)\n",
        "  korean_size = len(korean_data)\n",
        "  mandarin_size = len(mandarin_data)\n",
        "\n",
        "  from random import randint\n",
        "\n",
        "  english_vector = np.empty((0,1), int)\n",
        "  english_train_vector = np.empty((0,1), int)\n",
        "  english_test_vector = np.empty((0,1), int)\n",
        "  korean_vector = np.empty((0,1), int)\n",
        "  korean_train_vector = np.empty((0,1), int)\n",
        "  korean_test_vector = np.empty((0,1), int)\n",
        "  mandarin_vector = np.empty((0,1), int)\n",
        "  mandarin_train_vector = np.empty((0,1), int)\n",
        "  mandarin_test_vector = np.empty((0,1), int)\n",
        "\n",
        "  ## all the indexes of the objects available\n",
        "  for index in range (english_size):\n",
        "    english_vector = np.append(english_vector,index)\n",
        "  for index in range (korean_size):\n",
        "    korean_vector = np.append(korean_vector,index)\n",
        "  for index in range (mandarin_size):\n",
        "    mandarin_vector = np.append(mandarin_vector,index)\n",
        "\n",
        "  ## we randomly pick the train objects\n",
        "  for index in range (train_size):\n",
        "    value = randint(0, len(english_vector)-1)\n",
        "    english_train_vector = np.append(english_train_vector,english_vector[value])\n",
        "    english_vector = np.delete(english_vector,value)\n",
        "  for index in range (train_size):\n",
        "    value = randint(0, len(korean_vector)-1)\n",
        "    korean_train_vector = np.append(korean_train_vector,korean_vector[value])\n",
        "    korean_vector = np.delete(korean_vector,value)\n",
        "  for index in range (train_size):\n",
        "    value = randint(0, len(mandarin_vector)-1)\n",
        "    mandarin_train_vector = np.append(mandarin_train_vector,mandarin_vector[value])\n",
        "    mandarin_vector = np.delete(mandarin_vector,value)\n",
        "\n",
        "  ## we randomly pick the test objects\n",
        "  for index in range (test_size):\n",
        "    value = randint(0, len(english_vector)-1)\n",
        "    english_test_vector = np.append(english_test_vector,english_vector[value])\n",
        "    english_vector = np.delete(english_vector,value)\n",
        "  for index in range (test_size):\n",
        "    value = randint(0, len(korean_vector)-1)\n",
        "    korean_test_vector = np.append(korean_test_vector,korean_vector[value])\n",
        "    korean_vector = np.delete(korean_vector,value)\n",
        "  for index in range (test_size):\n",
        "    value = randint(0, len(mandarin_vector)-1)\n",
        "    mandarin_test_vector = np.append(mandarin_test_vector,mandarin_vector[value])\n",
        "    mandarin_vector = np.delete(mandarin_vector,value)\n",
        "\n",
        "  ## Now we add all the items to the dataframes\n",
        "  column_names = [\"filename\", \"native_language\"]\n",
        "\n",
        "  df_train = pd.DataFrame(columns = column_names)\n",
        "  df_test = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  ## First the test dataframe\n",
        "  for value in (english_test_vector):\n",
        "    df_test= df_test.append({'filename' : english_data['filename'][value] , 'native_language' : english_data['native_language'][value]} , ignore_index=True)\n",
        "  for value in (korean_test_vector):\n",
        "    df_test= df_test.append({'filename' : korean_data['filename'][value] , 'native_language' : korean_data['native_language'][value]} , ignore_index=True)\n",
        "  for value in (mandarin_test_vector):\n",
        "    df_test= df_test.append({'filename' : mandarin_data['filename'][value] , 'native_language' : mandarin_data['native_language'][value]} , ignore_index=True)\n",
        "\n",
        "  ## Then the train dataframe\n",
        "  for value in (english_train_vector):\n",
        "    df_train= df_train.append({'filename' : english_data['filename'][value] , 'native_language' : english_data['native_language'][value]} , ignore_index=True)\n",
        "  for value in (korean_train_vector):\n",
        "    df_train= df_train.append({'filename' : korean_data['filename'][value] , 'native_language' : korean_data['native_language'][value]} , ignore_index=True)\n",
        "  for value in (mandarin_train_vector):\n",
        "    df_train= df_train.append({'filename' : mandarin_data['filename'][value] , 'native_language' : mandarin_data['native_language'][value]} , ignore_index=True)\n",
        "\n",
        "  return df_train,df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYld61vudZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This function has the goal of creating the items of the train and test loaders\n",
        "def get_loaders():\n",
        "  train_loader = LanClassDataset(csv_file=df_train,\n",
        "                                      root_dir='/content/gdrive/My Drive/images/')\n",
        "  test_loader = LanClassDataset(csv_file=df_test,\n",
        "                                      root_dir='/content/gdrive/My Drive/images/')\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUSbNN7h6Fka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This network is the adaptation of the LeNet architecture.\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 5, 2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 15, 2)\n",
        "        self.fc1 = nn.Linear(15 * 7 * 15, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 15 * 7 * 15)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIR00pP4G-2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This network is the adaptation of the AlexNet architecture.\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 28, kernel_size=2, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(28, 84, kernel_size=2, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(84, 168, kernel_size=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(168, 112, kernel_size=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(112, 112, kernel_size=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(112 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj_v9cPX6HRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We initialize the network, the optimizer and the criterion.\n",
        "network = AlexNet()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum = momentum)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVaeFgol6Xqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i,_ in enumerate(train_loader):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    images = train_loader.dataset\n",
        "    labels = train_loader.labels\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = network(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # we print the loos obtained at the end of each train epoch\n",
        "    running_loss += loss.item()\n",
        "    if i == train_size*3-1:\n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "        (epoch, i + 1, running_loss / 20))\n",
        "    running_loss = 0.0\n",
        "\n",
        "  ## the state of the network is saved at the end of the train epoch\n",
        "  torch.save(network.state_dict(), F\"/content/gdrive/My Drive/model.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeiq7-vt6fgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for i,_ in enumerate(test_loader):\n",
        "        images = test_loader.dataset\n",
        "        labels = test_loader.labels\n",
        "        outputs = network(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the test images: %.2f %%' % (\n",
        "      100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNrYnsYbm-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch(it_number):\n",
        "  print('Epoch number %d:' % (it_number))\n",
        "  train(it_number)\n",
        "  test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiMAsF7k6imP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 5 simulations will be run, to select different random samples. Each of the selections\n",
        "## will be tested for each of the Networks + Optimizers defined, to compare the \n",
        "## performance of each combination.\n",
        "\n",
        "for simulation in range(1,6):\n",
        "  print('Simulation number %d:' % (simulation))\n",
        "  df_train,df_test = Random_selection_samples()\n",
        "  print('Random samples taken')\n",
        "  train_loader, test_loader = get_loaders()\n",
        "  print('Data loaded')\n",
        "\n",
        "  print('Network = Alexnet, Optimizer = SGD Mom = 0.8')\n",
        "  network = AlexNet()\n",
        "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                        momentum = momentum)\n",
        "  criterion = nn.CrossEntropyLoss() \n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Alexnet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())\n",
        "\n",
        "  print('Network = Lenet, Optimizer = Adam')\n",
        "  network = LeNet()\n",
        "  optimizer = optim.Adam(network.parameters(), lr=0.001,\n",
        "                         betas=(0.9, 0.999),weight_decay = 0.002, \n",
        "                         amsgrad=False)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())\n",
        "\n",
        "  print('Network = Lenet, Optimizer = RMSProp')\n",
        "  network = LeNet()\n",
        "  optimizer = optim.RMSprop(network.parameters(), lr=0.001,\n",
        "                          alpha=0.99)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())\n",
        "\n",
        "  print('Network = Lenet, Optimizer = SGD Mom = 0.8')\n",
        "  network = LeNet()\n",
        "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                        momentum = momentum)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())\n",
        "\n",
        "  print('Network = Lenet, Optimizer = SGD Mom = 0.4')\n",
        "  network = LeNet()\n",
        "  optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                        momentum = momentum_4)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())\n",
        "\n",
        "  print('Network = Lenet, Optimizer = SGD without momentum')\n",
        "  network = LeNet()\n",
        "  optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(datetime.datetime.now().time())\n",
        "  for epoch_number in range(1, n_epochs_Lenet + 1):\n",
        "    epoch(epoch_number)\n",
        "    print(datetime.datetime.now().time())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}